Rollout/Eval workflow

1) Rollout (Docker required): launches agents for each prepared task and records run dirs
   uv run python scripts/eval_rollouts.py run /Users/joshuapurtell/Documents/GitHub/one-shot-bench/configs/hello_world.toml
   - Outputs to: data/rollouts/<name>/<timestamp>/{manifest.json,runs.txt}
   - Each line of runs.txt is a run directory (data/runs/<run_id>) to evaluate

2) Eval (Docker required): for each run_dir, applies agent patch in container, runs pytest, and computes LM rubric
   For a single run:
     bash scripts/eval_in_docker.sh <run_dir> <prepared_task_dir>
   Or summarize latest rollout:
     uv run python scripts/eval_rollouts.py summarize hello_world --latest
   - Summary renders a table with unit score, LM score, tokens, tool calls

Notes
- You must save the run directory (data/runs/<run_id>) to evaluate it later
- eval_in_docker.sh relies on artifacts/{container_git_diff_from_baseline.patch|diff.patch}
